{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10850836,"sourceType":"datasetVersion","datasetId":6739209},{"sourceId":11262331,"sourceType":"datasetVersion","datasetId":7039231}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import and Install Dependancies","metadata":{}},{"cell_type":"code","source":"!pip install torch torchaudio transformers datasets scikit-learn soundfile torchvision audiomentations","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T16:54:22.340245Z","iopub.execute_input":"2025-04-12T16:54:22.340422Z","iopub.status.idle":"2025-04-12T16:55:45.238305Z","shell.execute_reply.started":"2025-04-12T16:54:22.340404Z","shell.execute_reply":"2025-04-12T16:55:45.237304Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torchaudio\nfrom transformers import HubertModel\nimport os\nimport glob\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T16:56:08.090504Z","iopub.execute_input":"2025-04-12T16:56:08.090833Z","iopub.status.idle":"2025-04-12T16:56:35.350948Z","shell.execute_reply.started":"2025-04-12T16:56:08.090804Z","shell.execute_reply":"2025-04-12T16:56:35.350032Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Declare Values","metadata":{}},{"cell_type":"code","source":"class Config:\n    def __init__(self):\n        self.datasets = {\n            # \"fake92\": \"/kaggle/input/imbalanceddataset/fake92\",\n            # \"real90\": \"/kaggle/input/imbalanceddataset/real90\", \n            # \"balanced\": \"/kaggle/input/fakes-and-reals/audio_train/audio_train\"\n        }\n        self.test_path = \"/kaggle/input/fakes-and-reals/audio_test/audio_test\"\n        \n        # Hardware settings\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.num_workers = 4\n        self.pin_memory = True\n        \n        # Training parameters\n        self.num_epochs = 20\n        self.batch_size = 8\n        self.train_val_split = 0.8\n        \n        # Audio processing\n        self.sample_rate = 16000\n        self.max_length = 16000\n        self.label_mapping = {\"real\": 0, \"fake\": 1}  # Simplified mapping\n        \n        # Model configuration\n        self.unfreeze_layers = [-1]  # Last layer only\n        self.use_augmentation = True\n        \n        # Optimization parameters\n        self.base_lr = 1e-5\n        self.classifier_lr_multiplier = 5\n        self.lr_decay_per_epoch = 0.95\n        self.min_lr = 1e-7\n        self.huber_weight_decay = 1e-5\n        self.classifier_weight_decay = 1e-4\n        self.gradient_clip = 1.0\n\nconfig = Config()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-12T16:58:02.426058Z","iopub.execute_input":"2025-04-12T16:58:02.427089Z","iopub.status.idle":"2025-04-12T16:58:02.433845Z","shell.execute_reply.started":"2025-04-12T16:58:02.427055Z","shell.execute_reply":"2025-04-12T16:58:02.432866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = {\n    'metrics': {},\n    'curves': {}\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T16:58:05.789873Z","iopub.execute_input":"2025-04-12T16:58:05.790349Z","iopub.status.idle":"2025-04-12T16:58:05.793969Z","shell.execute_reply.started":"2025-04-12T16:58:05.790325Z","shell.execute_reply":"2025-04-12T16:58:05.793423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare Audio Dataset","metadata":{}},{"cell_type":"code","source":"class AudioDataset(Dataset):\n    def __init__(self, root_dir, config, augment=False):\n        self.config = config\n        self.file_list = []\n        self.labels = []\n        self.augment = augment\n        self._load_data(root_dir)\n\n    def _is_valid_audio(self, file_path):\n        \"\"\"Enhanced validation with detailed logging\"\"\"\n        try:\n            # Check file size\n            if os.path.getsize(file_path) == 0:\n                print(f\"Empty file: {file_path}\")\n                return False\n                \n            # Try loading the file\n            waveform, sr = torchaudio.load(file_path)\n            if waveform.nelement() == 0:\n                return False\n            if waveform.shape[0] not in [1, 2]:  # Mono or stereo\n                return False\n            if waveform.shape[1] < 100:  # Minimum 100 samples\n                print(f\"Short audio: {file_path} ({waveform.shape[1]} samples)\")\n                return False\n            return True\n        except Exception as e:\n            print(f\"Error loading {file_path}: {str(e)}\")\n            return False\n\n    def _load_data(self, root_dir):\n        for label_name in self.config.label_mapping:\n            label = self.config.label_mapping[label_name]\n            folder = os.path.join(root_dir, label_name)\n            \n            if not os.path.exists(folder):\n                print(f\"Warning: Missing directory {folder}\")\n                continue\n                \n            files = glob.glob(os.path.join(folder, \"*.*\"))\n            print(f\"Found {len(files)} files in {folder}\")\n            \n            for file in files:\n                if self._is_valid_audio(file):\n                    self.file_list.append(file)\n                    self.labels.append(label)\n                else:\n                    print(f\"Warning: Skipping invalid file: {file}\")\n            \n\n        # Shuffle dataset\n        random.seed(42)\n        combined = list(zip(self.file_list, self.labels))\n        random.shuffle(combined)\n        self.file_list, self.labels = zip(*combined) if combined else ([], [])\n\n    def __getitem__(self, idx):\n        try:\n            waveform, sr = torchaudio.load(self.file_list[idx])\n            \n            # Resample if needed\n            if sr != self.config.sample_rate:\n                resampler = torchaudio.transforms.Resample(sr, self.config.sample_rate)\n                waveform = resampler(waveform)\n\n            # Process waveform\n            waveform = self._process_waveform(waveform)\n            return waveform, self.labels[idx]\n            \n        except Exception as e:\n            print(f\"Error loading {self.file_list[idx]}: {str(e)}\")\n            return torch.zeros((1, self.config.max_length)), 0\n\n    def _process_waveform(self, waveform):\n        # Convert to mono\n        if waveform.shape[0] > 1:\n            waveform = waveform.mean(dim=0, keepdim=True)\n            \n        # Trim/pad to fixed length\n        if waveform.shape[1] > self.config.max_length:\n            waveform = waveform[:, :self.config.max_length]\n        else:\n            pad_amount = self.config.max_length - waveform.shape[1]\n            waveform = torch.nn.functional.pad(waveform, (0, pad_amount))\n            \n        return waveform\n\n    def __len__(self):\n        return len(self.file_list)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T16:58:18.050726Z","iopub.execute_input":"2025-04-12T16:58:18.051298Z","iopub.status.idle":"2025-04-12T16:58:18.063941Z","shell.execute_reply.started":"2025-04-12T16:58:18.051268Z","shell.execute_reply":"2025-04-12T16:58:18.063004Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    waveforms, labels = [], []\n    for wav, lbl in batch:\n        waveforms.append(wav)\n        labels.append(lbl)\n    return torch.stack(waveforms), torch.tensor(labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T16:58:47.881119Z","iopub.execute_input":"2025-04-12T16:58:47.881966Z","iopub.status.idle":"2025-04-12T16:58:47.886918Z","shell.execute_reply.started":"2025-04-12T16:58:47.881930Z","shell.execute_reply":"2025-04-12T16:58:47.886039Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# HuBERT Classifier","metadata":{}},{"cell_type":"code","source":"class HuBERTClassifier(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.hubert = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\")\n        self._freeze_layers()\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(768, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, len(config.label_mapping))\n        )\n    \n    def _freeze_layers(self):\n        for idx, layer in enumerate(self.hubert.encoder.layers):\n            layer.requires_grad_(idx in config.unfreeze_layers)\n\n    def forward(self, x):\n        if x.dim() == 3:  # Handle channel dimension\n            x = x.squeeze(1)\n        outputs = self.hubert(x)\n        features = outputs.last_hidden_state.mean(dim=1)\n        return self.classifier(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T16:58:50.375966Z","iopub.execute_input":"2025-04-12T16:58:50.376670Z","iopub.status.idle":"2025-04-12T16:58:50.382321Z","shell.execute_reply.started":"2025-04-12T16:58:50.376632Z","shell.execute_reply":"2025-04-12T16:58:50.381600Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"test_dataset = AudioDataset(config.test_path, config)\ntest_loader = DataLoader(test_dataset, batch_size=config.batch_size,\n                         collate_fn=collate_fn, pin_memory=config.pin_memory)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T16:59:01.450666Z","iopub.execute_input":"2025-04-12T16:59:01.451393Z","iopub.status.idle":"2025-04-12T17:00:16.942761Z","shell.execute_reply.started":"2025-04-12T16:59:01.451359Z","shell.execute_reply":"2025-04-12T17:00:16.941835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add to top imports\nimport csv\nfrom datetime import datetime\nimport json\n\nclass DiskMetricWriter:\n    def __init__(self):\n        self.output_dir = \"/kaggle/working/metrics\"\n        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        os.makedirs(self.output_dir, exist_ok=True)\n        \n    def _get_path(self, dataset_name, metric_type):\n        return f\"{self.output_dir}/{dataset_name}_{self.timestamp}_{metric_type}.csv\"\n\n    def write_epoch_metrics(self, dataset_name, epoch, train_loss, val_loss, train_acc, val_acc):\n        path = self._get_path(dataset_name, \"training\")\n        write_header = not os.path.exists(path)\n        \n        with open(path, 'a', newline='') as f:\n            writer = csv.writer(f)\n            if write_header:\n                writer.writerow(['epoch', 'train_loss', 'val_loss', 'train_acc', 'val_acc'])\n            writer.writerow([epoch, train_loss, val_loss, train_acc, val_acc])\n\n    def write_final_metrics(self, dataset_name, test_loss, test_acc, auc_score):\n        path = self._get_path(dataset_name, \"final\")\n        with open(path, 'w', newline='') as f:\n            writer = csv.writer(f)\n            writer.writerow(['metric', 'value'])\n            writer.writerow(['test_loss', test_loss])\n            writer.writerow(['test_accuracy', test_acc])\n            writer.writerow(['auc', auc_score])\n\n    def write_confusion_matrix(self, dataset_name, cm, classes):\n        path = self._get_path(dataset_name, \"confusion\")\n        with open(path, 'w', newline='') as f:\n            writer = csv.writer(f)\n            writer.writerow([''] + list(classes))\n            for i, row in enumerate(cm):\n                writer.writerow([classes[i]] + list(row))\n\n    def write_classification_report(self, dataset_name, report):\n        path = self._get_path(dataset_name, \"classification\").replace('.csv', '.txt')\n        with open(path, 'w') as f:\n            f.write(report)\n\n    \n    def save_test_predictions(self, dataset_name, probs, labels):\n        probs_path = f\"{self.output_dir}/{dataset_name}_{self.timestamp}_probs.npy\"\n        labels_path = f\"{self.output_dir}/{dataset_name}_{self.timestamp}_labels.npy\"\n        np.save(probs_path, probs)\n        np.save(labels_path, labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T17:04:55.420013Z","iopub.execute_input":"2025-04-12T17:04:55.420282Z","iopub.status.idle":"2025-04-12T17:04:55.430506Z","shell.execute_reply.started":"2025-04-12T17:04:55.420263Z","shell.execute_reply":"2025-04-12T17:04:55.429749Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Metrics Saving and Visualisation","metadata":{}},{"cell_type":"code","source":"class TestVisualizer:\n    def __init__(self):\n        self.output_dir = \"/kaggle/working/test_metrics\"\n        os.makedirs(self.output_dir, exist_ok=True)\n        self.metrics = []\n\n    def add_metrics(self, dataset_name, test_metrics, fpr, tpr, auc_score, cm):\n        self.metrics.append({\n            'dataset': dataset_name,\n            'accuracy': test_metrics['accuracy'],\n            'loss': test_metrics['loss'],\n            'auc': auc_score,\n            'fpr': fpr,\n            'tpr': tpr,\n            'cm': cm\n        })\n\n    def save_metrics_to_csv(self):\n        df = pd.DataFrame([{\n            'dataset': m['dataset'],\n            'accuracy': m['accuracy'],\n            'loss': m['loss'],\n            'auc': m['auc']\n        } for m in self.metrics])\n        df.to_csv(f\"{self.output_dir}/test_metrics_summary.csv\", index=False)\n\n    def plot_all(self):\n        plt.figure(figsize=(20, 15))\n        \n        # 1. AUC-ROC Curves\n        plt.subplot(2, 2, 1)\n        for m in self.metrics:\n            plt.plot(m['fpr'], m['tpr'], label=f\"{m['dataset']} (AUC = {m['auc']:.2f})\")\n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('ROC Curves Comparison')\n        plt.legend()\n        \n        # 2. Accuracy Comparison\n        plt.subplot(2, 2, 2)\n        accuracies = [m['accuracy'] for m in self.metrics]\n        plt.bar(range(len(self.metrics)), accuracies)\n        plt.xticks(range(len(self.metrics)), [m['dataset'] for m in self.metrics])\n        plt.ylim(0, 1)\n        plt.ylabel('Accuracy')\n        plt.title('Test Accuracy Comparison')\n        \n        # 3. Loss Comparison\n        plt.subplot(2, 2, 3)\n        losses = [m['loss'] for m in self.metrics]\n        plt.bar(range(len(self.metrics)), losses)\n        plt.xticks(range(len(self.metrics)), [m['dataset'] for m in self.metrics])\n        plt.ylabel('Loss')\n        plt.title('Test Loss Comparison')\n        \n        # 4. Confusion Matrices\n        plt.subplot(2, 2, 4)\n        for idx, m in enumerate(self.metrics):\n            plt.subplot(2, 2, 4)\n            sns.heatmap(m['cm'], annot=True, fmt='d', \n                        xticklabels=['Real', 'Fake'], \n                        yticklabels=['Real', 'Fake'])\n            plt.title(f'Confusion Matrix - {m[\"dataset\"]}')\n            plt.ylabel('True Label')\n            plt.xlabel('Predicted Label')\n            plt.savefig(f\"{self.output_dir}/confusion_matrix_{m['dataset']}.png\")\n            plt.clf()\n        \n        plt.tight_layout()\n        plt.savefig(f\"{self.output_dir}/test_performance_summary.png\")\n        plt.close()\n\n# Initialize visualizer\ntest_visualizer = TestVisualizer()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T17:05:27.965646Z","iopub.execute_input":"2025-04-12T17:05:27.965960Z","iopub.status.idle":"2025-04-12T17:05:27.977243Z","shell.execute_reply.started":"2025-04-12T17:05:27.965940Z","shell.execute_reply":"2025-04-12T17:05:27.976356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metric_writer = DiskMetricWriter()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T17:05:30.899875Z","iopub.execute_input":"2025-04-12T17:05:30.900659Z","iopub.status.idle":"2025-04-12T17:05:30.904419Z","shell.execute_reply.started":"2025-04-12T17:05:30.900630Z","shell.execute_reply":"2025-04-12T17:05:30.903576Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, loader, device, criterion=None):\n    \"\"\"Enhanced evaluation function with all metrics\"\"\"\n    model.eval()\n    all_labels = []\n    all_preds = []\n    all_probs = []\n    total_loss = 0.0\n    \n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(inputs)\n            \n            # Calculate loss if criterion provided\n            if criterion:\n                loss = criterion(outputs, labels)\n                total_loss += loss.item()\n            \n            probs = torch.softmax(outputs, dim=1)\n            _, preds = torch.max(outputs, 1)\n            \n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy())\n    \n    metrics = {\n        'labels': np.array(all_labels),\n        'preds': np.array(all_preds),\n        'probs': np.array(all_probs)\n    }\n    \n    if criterion:\n        metrics['loss'] = total_loss / len(loader)\n        metrics['accuracy'] = np.mean(metrics['labels'] == metrics['preds'])\n    \n    return metrics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T17:05:33.730292Z","iopub.execute_input":"2025-04-12T17:05:33.731178Z","iopub.status.idle":"2025-04-12T17:05:33.741588Z","shell.execute_reply.started":"2025-04-12T17:05:33.731141Z","shell.execute_reply":"2025-04-12T17:05:33.740786Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# Training and evaluation loop for each dataset\nfor dataset_name, train_path in config.datasets.items():\n    print(f\"\\n{'='*40}\")\n    print(f\"Training on {dataset_name}\")\n    print(f\"{'='*40}\")\n    \n    # Initialize fresh model\n    model = HuBERTClassifier(config).to(config.device)\n    criterion = nn.CrossEntropyLoss()  # Fixed typo\n    optimizer = optim.Adam([\n        {'params': model.hubert.parameters(), 'lr': config.base_lr},\n        {'params': model.classifier.parameters(), \n         'lr': config.base_lr * config.classifier_lr_multiplier}\n    ])\n    \n    # Create datasets\n    train_dataset = AudioDataset(train_path, config, augment=True)\n    train_size = int(config.train_val_split * len(train_dataset))\n    train_dataset, val_dataset = random_split(train_dataset, [train_size, len(train_dataset)-train_size])\n    \n    train_loader = DataLoader(train_dataset, batch_size=config.batch_size,\n                            shuffle=True, collate_fn=collate_fn)\n    val_loader = DataLoader(val_dataset, batch_size=config.batch_size,\n                          collate_fn=collate_fn)\n    \n    best_val_acc = 0.0\n    \n    # Single epoch loop\n    for epoch in range(config.num_epochs):\n        model.train()\n        epoch_loss = 0.0\n        correct = 0\n        total = 0\n        \n        # Training\n        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n            inputs, labels = inputs.to(config.device), labels.to(config.device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(config.device), labels.to(config.device)\n                outputs = model(inputs)\n                val_loss += criterion(outputs, labels).item()\n                \n                _, predicted = torch.max(outputs.data, 1)\n                val_total += labels.size(0)\n                val_correct += (predicted == labels).sum().item()\n        \n        # Calculate metrics\n        train_loss = epoch_loss / len(train_loader)\n        train_acc = correct / total\n        val_loss = val_loss / len(val_loader)\n        val_acc = val_correct / val_total\n        \n        # Write to disk immediately\n        metric_writer.write_epoch_metrics(\n            dataset_name=dataset_name,\n            epoch=epoch+1,\n            train_loss=train_loss,\n            val_loss=val_loss,\n            train_acc=train_acc,\n            val_acc=val_acc\n        )\n        \n        # Update learning rate\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = max(param_group['lr'] * config.lr_decay_per_epoch, \n                                  config.min_lr)\n        \n        # Save best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), f\"best_model_{dataset_name}.pth\")\n        \n        print(f\"Epoch {epoch+1}/{config.num_epochs}\")\n        print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.2%}\")\n        print(f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.2%}\\n\")\n    \n    # Final test evaluation\n    test_metrics = evaluate_model(model, test_loader, config.device, criterion)\n\n\n    metric_writer.save_test_predictions(\n        dataset_name=dataset_name,\n        probs=test_metrics['probs'],\n        labels=test_metrics['labels']\n    )\n    \n    # Final test evaluation\n    test_metrics = evaluate_model(model, test_loader, config.device, criterion)\n    \n    # Calculate metrics\n    fpr, tpr, _ = roc_curve(test_metrics['labels'], test_metrics['probs'][:, 1])\n    auc_score = auc(fpr, tpr)\n    cm = confusion_matrix(test_metrics['labels'], np.argmax(test_metrics['probs'], axis=1))\n    \n    # Store metrics for visualization\n    test_visualizer.add_metrics(dataset_name, test_metrics, fpr, tpr, auc_score, cm)\n    \n    metric_writer.write_final_metrics(\n        dataset_name=dataset_name,\n        test_loss=test_metrics['loss'],\n        test_acc=test_metrics['accuracy'],\n        auc_score=auc_score\n    )\n    \n    # Cleanup\n    del model, criterion, optimizer, train_dataset, val_dataset\n    torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T17:05:39.566738Z","iopub.execute_input":"2025-04-12T17:05:39.567065Z","iopub.status.idle":"2025-04-12T20:25:27.809178Z","shell.execute_reply.started":"2025-04-12T17:05:39.567042Z","shell.execute_reply":"2025-04-12T20:25:27.808492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# After all datasets are processed\ntest_visualizer.save_metrics_to_csv()\ntest_visualizer.plot_all()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:25:43.394111Z","iopub.execute_input":"2025-04-12T20:25:43.394670Z","iopub.status.idle":"2025-04-12T20:25:44.403391Z","shell.execute_reply.started":"2025-04-12T20:25:43.394634Z","shell.execute_reply":"2025-04-12T20:25:44.402384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_comparative_curves(metric_dir=\"/kaggle/working/metrics\"):\n    import os\n    import glob\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.metrics import roc_curve\n    \n    # Create directory if not exists\n    os.makedirs(metric_dir, exist_ok=True)\n    \n    # Get all dataset names from config\n    datasets = list(config.datasets.keys())\n    \n    plt.figure(figsize=(15, 10))\n    \n    # 1. Loss Comparison\n    plt.subplot(2, 2, 1)\n    for dataset in datasets:\n        try:\n            # Find latest training file for this dataset\n            train_files = glob.glob(f\"{metric_dir}/{dataset}_*_training.csv\")\n            if not train_files:\n                print(f\"No training files found for {dataset}, skipping\")\n                continue\n                \n            latest_file = max(train_files, key=os.path.getctime)\n            df = pd.read_csv(latest_file)\n            \n            plt.plot(df['epoch'], df['train_loss'], label=f'{dataset} Train')\n            plt.plot(df['epoch'], df['val_loss'], '--', label=f'{dataset} Val')\n        except Exception as e:\n            print(f\"Error plotting {dataset} loss: {str(e)}\")\n            continue\n\n    plt.title('Loss Comparison')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # 2. Accuracy Comparison\n    plt.subplot(2, 2, 2)\n    for dataset in datasets:\n        try:\n            train_files = glob.glob(f\"{metric_dir}/{dataset}_*_training.csv\")\n            if not train_files:\n                continue\n                \n            latest_file = max(train_files, key=os.path.getctime)\n            df = pd.read_csv(latest_file)\n            \n            plt.plot(df['epoch'], df['train_acc'], label=f'{dataset} Train')\n            plt.plot(df['epoch'], df['val_acc'], '--', label=f'{dataset} Val')\n        except Exception as e:\n            print(f\"Error plotting {dataset} accuracy: {str(e)}\")\n            continue\n\n    plt.title('Accuracy Comparison')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    # 3. Test Metrics Comparison\n    plt.subplot(2, 2, 3)\n    metrics_list = []\n    for dataset in datasets:\n        try:\n            test_files = glob.glob(f\"{metric_dir}/{dataset}_*_final.csv\")\n            if not test_files:\n                continue\n                \n            latest_test = max(test_files, key=os.path.getctime)\n            df_test = pd.read_csv(latest_test)\n            \n            accuracy_value = df_test.loc[df_test['metric'] == 'test_accuracy', 'value'].values[0]\n            loss_value = df_test.loc[df_test['metric'] == 'test_loss', 'value'].values[0]\n            \n            metrics_list.append({\n                'dataset': dataset,\n                'accuracy': accuracy_value,\n                'loss': loss_value\n            })\n        except Exception as e:\n            print(f\"Error loading test metrics for {dataset}: {str(e)}\")\n            continue\n\n    if metrics_list:\n        metrics_df = pd.DataFrame(metrics_list)\n        x = np.arange(len(metrics_df))\n        width = 0.35\n        \n        plt.bar(x - width/2, metrics_df['accuracy'], width, label='Accuracy')\n        plt.bar(x + width/2, metrics_df['loss'], width, label='Loss')\n        plt.xticks(x, metrics_df['dataset'])\n        plt.title('Test Set Performance')\n        plt.legend()\n    \n    # 4. ROC Comparison\n    plt.subplot(2, 2, 4)\n    for dataset in datasets:\n        try:\n            # Get latest test files\n            test_files = glob.glob(f\"{metric_dir}/{dataset}_*_final.csv\")\n            prob_files = glob.glob(f\"{metric_dir}/{dataset}_*_probs.npy\")\n            label_files = glob.glob(f\"{metric_dir}/{dataset}_*_labels.npy\")\n            \n            if not test_files or not prob_files or not label_files:\n                continue\n                \n            # Load data\n            latest_test = max(test_files, key=os.path.getctime)\n            latest_probs = max(prob_files, key=os.path.getctime)\n            latest_labels = max(label_files, key=os.path.getctime)\n            \n            df_test = pd.read_csv(latest_test)\n            auc_score = df_test.loc[df_test['metric'] == 'auc', 'value'].values[0]\n            probs = np.load(latest_probs)\n            labels = np.load(latest_labels)\n            \n            fpr, tpr, _ = roc_curve(labels, probs[:, 1])\n            plt.plot(fpr, tpr, label=f'{dataset} (AUC = {auc_score:.2f})')\n        except Exception as e:\n            print(f\"Error plotting ROC for {dataset}: {str(e)}\")\n            continue\n\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.title('ROC Curve Comparison')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig(f'{metric_dir}/comparison_plot.png')\n    plt.close()\n    print(f\"Comparison plot saved to {metric_dir}/comparison_plot.png\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:26:08.152171Z","iopub.execute_input":"2025-04-12T20:26:08.152782Z","iopub.status.idle":"2025-04-12T20:26:08.174414Z","shell.execute_reply.started":"2025-04-12T20:26:08.152750Z","shell.execute_reply":"2025-04-12T20:26:08.173555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate comparison plots\nplot_comparative_curves()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:26:16.613106Z","iopub.execute_input":"2025-04-12T20:26:16.613711Z","iopub.status.idle":"2025-04-12T20:26:17.370192Z","shell.execute_reply.started":"2025-04-12T20:26:16.613683Z","shell.execute_reply":"2025-04-12T20:26:17.369491Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Download Files","metadata":{}},{"cell_type":"code","source":"# zip metrics\n# zip test_metrics\n\n# download link for best_model_{dataset_name}.pth and all metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r metrics.zip /kaggle/working/metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:26:22.621889Z","iopub.execute_input":"2025-04-12T20:26:22.622158Z","iopub.status.idle":"2025-04-12T20:26:22.836642Z","shell.execute_reply.started":"2025-04-12T20:26:22.622138Z","shell.execute_reply":"2025-04-12T20:26:22.835586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r test_metrics.zip /kaggle/working/test-metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:26:26.097660Z","iopub.execute_input":"2025-04-12T20:26:26.098637Z","iopub.status.idle":"2025-04-12T20:26:26.288700Z","shell.execute_reply.started":"2025-04-12T20:26:26.098601Z","shell.execute_reply":"2025-04-12T20:26:26.287734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'best_model_fake92.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:26:28.807392Z","iopub.execute_input":"2025-04-12T20:26:28.807982Z","iopub.status.idle":"2025-04-12T20:26:28.813933Z","shell.execute_reply.started":"2025-04-12T20:26:28.807951Z","shell.execute_reply":"2025-04-12T20:26:28.813113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FileLink(r'best_model_real90.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:26:32.797633Z","iopub.execute_input":"2025-04-12T20:26:32.797957Z","iopub.status.idle":"2025-04-12T20:26:32.803857Z","shell.execute_reply.started":"2025-04-12T20:26:32.797937Z","shell.execute_reply":"2025-04-12T20:26:32.802939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FileLink(r'best_model_balanced.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:26:35.243255Z","iopub.execute_input":"2025-04-12T20:26:35.244081Z","iopub.status.idle":"2025-04-12T20:26:35.249099Z","shell.execute_reply.started":"2025-04-12T20:26:35.244054Z","shell.execute_reply":"2025-04-12T20:26:35.248278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FileLink(r'metrics.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:26:37.777018Z","iopub.execute_input":"2025-04-12T20:26:37.777806Z","iopub.status.idle":"2025-04-12T20:26:37.783029Z","shell.execute_reply.started":"2025-04-12T20:26:37.777781Z","shell.execute_reply":"2025-04-12T20:26:37.782347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FileLink(r'test_metrics.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}