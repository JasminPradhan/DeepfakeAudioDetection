{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10850836,"sourceType":"datasetVersion","datasetId":6739209},{"sourceId":11262331,"sourceType":"datasetVersion","datasetId":7039231}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install and Import Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install torch torchaudio transformers datasets scikit-learn soundfile torchvision\n!pip install -U ray\n!pip install -U \"flwr[simulation]==1.15.2\"\n!pip install audiomentations optuna opacus ipython","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:53:52.400492Z","iopub.execute_input":"2025-04-07T00:53:52.400712Z","iopub.status.idle":"2025-04-07T00:54:59.423125Z","shell.execute_reply.started":"2025-04-07T00:53:52.400691Z","shell.execute_reply":"2025-04-07T00:54:59.422260Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport logging\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchaudio\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport flwr as fl\nfrom transformers import HubertModel, HubertConfig\nimport ray","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:54:59.424072Z","iopub.execute_input":"2025-04-07T00:54:59.424288Z","iopub.status.idle":"2025-04-07T00:55:27.068723Z","shell.execute_reply.started":"2025-04-07T00:54:59.424269Z","shell.execute_reply":"2025-04-07T00:55:27.068030Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Setup and Values","metadata":{}},{"cell_type":"code","source":"class Config:\n    def __init__(self):\n        self.dirs = {\n            # \"train\": \"/kaggle/input/imbalanceddataset/real90\",\n            # \"train\": \"/kaggle/input/imbalanceddataset/fake92\", #use it for fake92 \n            # \"train\": \"/kaggle/input/fakes-and-reals/audio_train/audio_train\",# use it for balanced\n            \"test\": \"/kaggle/input/fakes-and-reals/audio_test/audio_test\",\n        }\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.num_clients = 4\n        self.num_rounds = 20\n        self.epochs_per_round = 1\n        self.batch_size = 8\n        self.sample_rate = 16000\n        self.max_length = 16000\n        self.label_mapping = {\"real\": 0, \"fake\": 1, \"REAL\":0, \"FAKE\":1}\n        self.unfreeze_layers = [-1]  # Last transformer layer\n        self.base_lr = 1e-5\n        self.classifier_lr_multiplier = 10\n        self.lr_decay = 0.95\n        self.min_lr = 1e-7\n\nconfig = Config()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:55:27.069564Z","iopub.execute_input":"2025-04-07T00:55:27.070199Z","iopub.status.idle":"2025-04-07T00:55:27.155631Z","shell.execute_reply.started":"2025-04-07T00:55:27.070168Z","shell.execute_reply":"2025-04-07T00:55:27.154649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\nimport torchaudio\n\nclass AudioAugmenter:\n    def __init__(self, sample_rate=16000):\n        self.augment = Compose([\n            AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.005, p=0.2),\n            TimeStretch(min_rate=0.95, max_rate=1.05, p=0.2),  # More conservative stretching\n            PitchShift(min_semitones=-1, max_semitones=1, p=0.2),  # Reduced semitone range\n            Shift(min_shift=-0.1, max_shift=0.1, p=0.2),\n        ])\n        self.sample_rate = sample_rate\n        \n    def __call__(self, waveform):\n        \"\"\"Process and augment waveform while maintaining proper dimensions\"\"\"\n        # Convert to numpy and ensure proper shape\n        np_waveform = waveform.numpy()\n        \n        # Handle different channel configurations\n        if np_waveform.ndim == 2:  # [channels, time]\n            # Convert multi-channel to mono by averaging\n            np_waveform = np.mean(np_waveform, axis=0)\n        elif np_waveform.ndim == 1:  # [time]\n            pass  # Already mono\n        else:\n            raise ValueError(f\"Unexpected waveform shape: {np_waveform.shape}\")\n        \n        # Apply augmentations\n        augmented = self.augment(\n            samples=np_waveform,\n            sample_rate=self.sample_rate\n        )\n        \n        # Convert back to tensor with proper dimensions [1, time]\n        return torch.from_numpy(augmented).unsqueeze(0).float()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:57:28.564383Z","iopub.execute_input":"2025-04-07T00:57:28.564737Z","iopub.status.idle":"2025-04-07T00:57:28.740117Z","shell.execute_reply.started":"2025-04-07T00:57:28.564710Z","shell.execute_reply":"2025-04-07T00:57:28.739274Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading Dataset","metadata":{}},{"cell_type":"markdown","source":"> **Audiomentations library used to impart noise and other impairments to our audio samples. To train on the original audio samples comment out the parts in the below code where its commented \"#Impairments\".**","metadata":{}},{"cell_type":"code","source":"class AudioDataset(Dataset):\n    def __init__(self, root_dir, config, augment=False):\n        self.config = config\n        self.file_list = []\n        self.labels = []\n        self.augment = augment #Impairments\n        self._load_data(root_dir)\n        self.augmenter = AudioAugmenter(sample_rate=config.sample_rate) if augment else None\n        \n    def _load_data(self, root_dir):\n        \"\"\"Improved data loading with better error handling\"\"\"\n        for label_name, label in config.label_mapping.items():\n            folder = os.path.join(root_dir, label_name)\n            if not os.path.exists(folder):\n                print(f\"Warning: Missing directory: {folder}\")\n                continue\n                \n            files = glob.glob(os.path.join(folder, \"*.*\"))\n            print(f\"Found {len(files)} files in {folder}\")\n            \n            for file in files:\n                if self._is_valid_audio(file):\n                    self.file_list.append(file)\n                    self.labels.append(label)\n                else:\n                    print(f\"Warning: Skipping invalid file: {file}\")\n\n\n        # Shuffle with seed for reproducibility\n        random.seed(42)\n        combined = list(zip(self.file_list, self.labels))\n        random.shuffle(combined)\n        self.file_list, self.labels = zip(*combined) if combined else ([], [])\n\n    def _is_valid_audio(self, file_path):\n        \"\"\"Enhanced validation with detailed logging\"\"\"\n        try:\n            # Check file size\n            if os.path.getsize(file_path) == 0:\n                print(f\"Empty file: {file_path}\")\n                return False\n                \n            # Try loading the file\n            waveform, sr = torchaudio.load(file_path)\n            if waveform.nelement() == 0:\n                return False\n            if waveform.shape[0] not in [1, 2]:  # Mono or stereo\n                return False\n            if waveform.shape[1] < 100:  # Minimum 100 samples\n                print(f\"Short audio: {file_path} ({waveform.shape[1]} samples)\")\n                return False\n            return True\n        except Exception as e:\n            print(f\"Error loading {file_path}: {str(e)}\")\n            return False\n\n    def __getitem__(self, idx):\n        try:\n            waveform, sr = torchaudio.load(self.file_list[idx])\n            \n            # Resample if necessary\n            if sr != config.sample_rate:\n                resampler = torchaudio.transforms.Resample(sr, config.sample_rate)\n                waveform = resampler(waveform)\n\n            if self.augment and self.augmenter: #Impairments\n                waveform = self.augmenter(waveform) #Impairments\n\n            # Convert to mono and process\n            waveform = self._process_waveform(waveform)\n            label = self.labels[idx]\n\n            return waveform.squeeze(0), label\n        except Exception as e:\n            print(f\"Error processing {self.file_list[idx]}: {str(e)}\")\n            return torch.zeros((1, config.max_length)), 0  # Return dummy data\n\n    def _process_waveform(self, waveform):\n        \"\"\"Guarantee 2D output [1, max_length] regardless of input\"\"\"\n        # Convert to 2D if needed\n        if waveform.dim() == 1:\n            waveform = waveform.unsqueeze(0)  # [1, time]\n        elif waveform.dim() > 2:\n            waveform = waveform.view(-1, waveform.size(-1))  # Flatten to 2D\n        \n        # Convert to mono\n        if waveform.size(0) > 1:\n            waveform = waveform.mean(dim=0, keepdim=True)\n\n        # Trim/pad to exact length\n        if waveform.size(1) > self.config.max_length:\n            waveform = waveform[:, :self.config.max_length]\n        else:\n            pad_amount = self.config.max_length - waveform.size(1)\n            waveform = torch.nn.functional.pad(waveform, (0, pad_amount))\n            \n        return waveform  # Guaranteed [1, max_length]\n\n    def __len__(self):\n        \"\"\"Returns the total number of samples in the dataset\"\"\"\n        return len(self.file_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:57:35.568097Z","iopub.execute_input":"2025-04-07T00:57:35.568768Z","iopub.status.idle":"2025-04-07T00:57:35.581065Z","shell.execute_reply.started":"2025-04-07T00:57:35.568737Z","shell.execute_reply":"2025-04-07T00:57:35.580304Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## HuBERT Classification","metadata":{}},{"cell_type":"code","source":"class HuBERTClassifier(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        \n        # Load pre-trained HuBERT\n        self.hubert = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\")\n        self._freeze_layers()\n        \n        # Classification head\n        self.classifier = nn.Sequential(\n            nn.Linear(768, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, len(config.label_mapping))\n        )\n\n    def _freeze_layers(self):\n        \"\"\"Improved layer freezing with logging\"\"\"\n        total_layers = len(self.hubert.encoder.layers)\n        print(f\"Total HuBERT layers: {total_layers}\")\n        \n        for i, layer in enumerate(self.hubert.encoder.layers):\n            if i not in self.config.unfreeze_layers:\n                for param in layer.parameters():\n                    param.requires_grad = False\n            else:\n                print(f\"Unfreezing layer {i}\")\n\n    def forward(self, input_values):\n        \"\"\"Handle [batch, channels, time] input\"\"\"\n        # Convert to HuBERT-compatible 2D [batch, time]\n        if input_values.dim() == 3:\n            input_values = input_values.squeeze(1)  # Remove channel dim\n            \n        outputs = self.hubert(input_values)\n        pooled_output = outputs.last_hidden_state.mean(dim=1)\n        return self.classifier(pooled_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:57:35.582123Z","iopub.execute_input":"2025-04-07T00:57:35.582319Z","iopub.status.idle":"2025-04-07T00:57:35.603246Z","shell.execute_reply.started":"2025-04-07T00:57:35.582302Z","shell.execute_reply":"2025-04-07T00:57:35.602669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"Robust collate function handling various audio dimensions\"\"\"\n    # 1. Filter invalid entries\n    batch = [b for b in batch if b is not None]\n    \n    if not batch:\n        return torch.zeros((0, 1, config.max_length)), torch.zeros(0, dtype=torch.long)\n    \n    # 2. Separate components\n    waveforms, labels = zip(*batch)\n    \n    waveforms = torch.stack(waveforms)  # [batch, 1, max_length]\n    labels = torch.tensor(labels, dtype=torch.long)\n    \n    return waveforms, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:57:35.604640Z","iopub.execute_input":"2025-04-07T00:57:35.604833Z","iopub.status.idle":"2025-04-07T00:57:35.622647Z","shell.execute_reply.started":"2025-04-07T00:57:35.604816Z","shell.execute_reply":"2025-04-07T00:57:35.621875Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Metrics Saving","metadata":{}},{"cell_type":"code","source":"import json\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\nclass DiskMetricsCollector:\n    def __init__(self, base_dir=\"/kaggle/working/metrics\"):\n        self.base_dir = Path(base_dir)\n        self.base_dir.mkdir(parents=True, exist_ok=True)\n        self.metrics_file = self.base_dir / \"all_metrics.json\"\n        \n        # Initialize empty metrics structure if file doesn't exist\n        if not self.metrics_file.exists():\n            with open(self.metrics_file, \"w\") as f:\n                json.dump({}, f)\n    \n    def add_metrics(self, round_num, client_id, metrics):\n        \"\"\"Append metrics to consolidated JSON file\"\"\"\n        # Load existing data\n        with open(self.metrics_file, \"r\") as f:\n            all_metrics = json.load(f)\n        \n        # Create round entry if not exists\n        round_key = f\"round_{round_num}\"\n        if round_key not in all_metrics:\n            all_metrics[round_key] = {}\n        \n        # Add client metrics\n        client_key = f\"client_{client_id}\"\n        all_metrics[round_key][client_key] = {\n            \"loss\": metrics[\"loss\"],\n            \"accuracy\": metrics[\"accuracy\"]\n        }\n        \n        # Save back to file\n        with open(self.metrics_file, \"w\") as f:\n            json.dump(all_metrics, f, indent=2)\n\n    def plot_round(self, round_num):\n        \"\"\"Plot metrics directly from JSON\"\"\"\n        with open(self.metrics_file, \"r\") as f:\n            all_metrics = json.load(f)\n        \n        round_key = f\"round_{round_num}\"\n        if round_key not in all_metrics:\n            print(f\"No metrics for round {round_num}\")\n            return\n        \n        plt.figure(figsize=(12, 6))\n        \n        for client_key, metrics in all_metrics[round_key].items():\n            client_id = client_key.split(\"_\")[1]\n            epochs = range(1, len(metrics[\"loss\"]) + 1)\n            \n            plt.subplot(1, 2, 1)\n            plt.plot(epochs, metrics[\"loss\"], label=f'Client {client_id}')\n            plt.title(f'Round {round_num} - Loss')\n            plt.xlabel('Epoch')\n            \n            plt.subplot(1, 2, 2)\n            plt.plot(epochs, metrics[\"accuracy\"], label=f'Client {client_id}')\n            plt.title(f'Round {round_num} - Accuracy')\n            plt.xlabel('Epoch')\n        \n        plt.legend()\n        plt.tight_layout()\n        plt.show()\n        plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:57:35.623340Z","iopub.execute_input":"2025-04-07T00:57:35.623547Z","iopub.status.idle":"2025-04-07T00:57:35.639801Z","shell.execute_reply.started":"2025-04-07T00:57:35.623529Z","shell.execute_reply":"2025-04-07T00:57:35.639211Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Federated Setup","metadata":{}},{"cell_type":"code","source":"import gc\nclass FlowerClient(fl.client.NumPyClient):\n    def __init__(self, model, train_data, test_data, config, client_id,metrics_collector):\n        self.client_id = client_id\n        self.model = model.to(config.device)\n        self.config = config\n        self.server_round = 0\n        self.train_loader = DataLoader(\n            train_data,\n            batch_size=config.batch_size,\n            shuffle=True,\n            collate_fn=collate_fn,\n            drop_last=True\n        )\n        self.test_loader = DataLoader(\n            test_data,\n            batch_size=config.batch_size,\n            collate_fn=collate_fn\n        )\n        self.optimizer = self._create_optimizer()\n        self.criterion = nn.CrossEntropyLoss()\n        self.metrics_collector = metrics_collector\n\n    \n    # def get_parameters(self, config):\n    #     \"\"\"Proper parameter serialization using state_dict\"\"\"\n    #     return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n\n    \n    # def set_parameters(self, parameters):\n    #     \"\"\"Proper parameter deserialization using state_dict\"\"\"\n    #     params_dict = zip(self.model.state_dict().keys(), parameters)\n    #     state_dict = {k: torch.tensor(v) for k, v in params_dict}\n    #     self.model.load_state_dict(state_dict, strict=True)\n    \n    \n    def _create_optimizer(self):\n        \"\"\"Optimizer with learning rate decay and stability features\"\"\"\n        decay_factor = max(\n            self.config.lr_decay ** self.server_round,\n            self.config.min_lr / self.config.base_lr\n        )\n        \n        params = [\n            {\n                \"params\": self.model.hubert.parameters(),\n                \"lr\": self.config.base_lr * decay_factor,\n                \"weight_decay\": 1e-5\n            },\n            {\n                \"params\": self.model.classifier.parameters(),\n                \"lr\": (self.config.base_lr * self.config.classifier_lr_multiplier) * decay_factor,\n                \"weight_decay\": 1e-4\n            }\n        ]\n        \n        return optim.Adam(\n            params,\n            eps=1e-7,\n            amsgrad=True  # Improved convergence stability\n        )\n    \n    def get_parameters(self, config):\n        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n\n    \n    def set_parameters(self, parameters):\n        if any(np.isnan(p).any() for p in parameters):\n            raise ValueError(f\"Client {self.client_id} received NaN parameters\")\n            \n        params_dict = zip(self.model.state_dict().keys(), parameters)\n        state_dict = {\n            k: torch.tensor(v).to(self.config.device)\n            for k, v in params_dict\n        }\n        self.model.load_state_dict(state_dict, strict=True)\n\n        # if not parameters:\n        #     raise ValueError(\"Received empty parameters\")\n        \n        # try:\n        #     # Convert numpy arrays to tensors\n        #     params_dict = zip(self.model.state_dict().keys(), parameters)\n        #     state_dict = {\n        #         k: torch.tensor(v).to(self.config.device)\n        #         for k, v in params_dict\n        #     }\n            \n        #     # Strict loading with informative errors\n        #     self.model.load_state_dict(state_dict, strict=True)\n            \n        # except RuntimeError as e:\n        #     # Wrap PyTorch errors for Flower compatibility\n        #     raise fl.common.parameter.ParametersError(str(e)) from e\n\n    \n    def fit(self, parameters, config):\n        self.set_parameters(parameters)\n        self.server_round = config.get(\"server_round\", 1)\n        self.optimizer = self._create_optimizer()  # Update optimizer for current round\n        self.set_parameters(parameters)\n        self.model.train()\n    \n        # Track per-epoch metrics\n        epoch_losses = []\n        epoch_accuracies = []\n        epoch_metrics = {'loss': [], 'accuracy': []}\n    \n        try:\n            if len(self.train_loader.dataset) == 0:\n                print(f\"Client has no training data!\")\n                return (\n                    self.get_parameters({}),\n                    0,\n                    {\"loss\": 0.0, \"accuracy\": 0.0}\n                )\n    \n            for epoch in range(self.config.epochs_per_round):\n                epoch_loss = 0.0\n                correct = 0\n                total = 0\n                progress_bar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}\")\n    \n                for batch_idx, (data, targets) in enumerate(progress_bar):\n                    data, targets = data.to(self.config.device), targets.to(self.config.device)\n                    \n                    self.optimizer.zero_grad()\n                    outputs = self.model(data)\n\n                    if torch.isnan(outputs).any():\n                        print(f\"NaN outputs detected, skipping batch\")\n                        continue\n                    loss = self.criterion(outputs, targets)\n\n                    if torch.isnan(loss):\n                        print(f\"NaN loss detected, resetting parameters\")\n                        self.set_parameters(parameters)  # Reset to server parameters\n                        return self.get_parameters({}), 0, {}\n                        \n                    loss.backward()\n\n                    torch.nn.utils.clip_grad_norm_(\n                        self.model.parameters(),\n                        max_norm=1.0,\n                        norm_type=2.0\n                    )\n                    \n                    self.optimizer.step()\n    \n                    epoch_loss += loss.item()\n                    preds = outputs.argmax(dim=1)\n                    correct += (preds == targets).sum().item()\n                    total += targets.size(0)\n    \n                    progress_bar.set_postfix(loss=loss.item())\n                    \n                    del outputs, loss\n                    torch.cuda.empty_cache()\n                    gc.collect()\n    \n                # Calculate metrics properly\n                if len(self.train_loader) > 0 and total > 0:\n                    avg_loss = epoch_loss / len(self.train_loader)\n                    accuracy = correct / total\n                else:  # Handle empty/corrupted data cases\n                    avg_loss = 0.0\n                    accuracy = 0.0\n    \n                epoch_losses.append(avg_loss)\n                epoch_accuracies.append(accuracy)\n                epoch_metrics['loss'].append(avg_loss)\n                epoch_metrics['accuracy'].append(accuracy)  # Fixed variable name\n    \n        except Exception as e:\n            print(f\"Training error: {str(e)}\")\n            return self.get_parameters({}), 0, {}\n    \n        # Store metrics\n        self.metrics_collector.add_metrics(\n            self.server_round,  # Use the server_round from config\n            self.client_id,\n            {'loss': epoch_losses, 'accuracy': epoch_accuracies}\n        )\n    \n        # Calculate averages\n        avg_loss = sum(epoch_losses)/len(epoch_losses) if epoch_losses else 0.0\n        avg_accuracy = sum(epoch_accuracies)/len(epoch_accuracies) if epoch_accuracies else 0.0\n    \n        return (\n            self.get_parameters({}), \n            len(self.train_loader.dataset),\n            {\"loss\": float(avg_loss), \"accuracy\": float(avg_accuracy)}\n        )\n\n    def evaluate(self, parameters, config):\n        self.set_parameters(parameters)\n        self.model.eval()\n        \n        total_loss = 0\n        correct = 0\n        with torch.no_grad():\n            for data, targets in self.test_loader:\n                data, targets = data.to(self.config.device), targets.to(self.config.device)\n                outputs = self.model(data)\n                total_loss += self.criterion(outputs, targets).item()\n                preds = outputs.argmax(dim=1)\n                correct += (preds == targets).sum().item()\n\n        accuracy = correct / len(self.test_loader.dataset)\n        avg_loss = total_loss / len(self.test_loader)\n        return (\n            float(avg_loss), \n            len(self.test_loader.dataset), \n            {\n                \"loss\":float(avg_loss),\n                \"accuracy\": float(accuracy)\n            })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:57:35.725153Z","iopub.execute_input":"2025-04-07T00:57:35.725375Z","iopub.status.idle":"2025-04-07T00:57:35.743306Z","shell.execute_reply.started":"2025-04-07T00:57:35.725357Z","shell.execute_reply":"2025-04-07T00:57:35.742493Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomFedAvg(fl.server.strategy.FedAvg):\n    def __init__(self, metrics_collector, initial_parameters=None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.metrics_collector = metrics_collector\n        self.current_parameters = initial_parameters  # Initialize with provided parameters\n        self.initial_parameters = initial_parameters  # Store initial parameters\n\n    def aggregate_fit(self, server_round, results, failures):\n        # Call parent aggregation first\n        aggregated = super().aggregate_fit(server_round, results, failures)\n        \n        if aggregated is not None:\n            # Store the aggregated parameters\n            self.current_parameters = aggregated[0]\n        elif self.current_parameters is None:\n            # Fallback to initial parameters if no aggregation happened\n            self.current_parameters = self.initial_parameters\n            \n        return aggregated\n\n    def get_parameters(self, config):\n        # Return current parameters or initial ones if none exist\n        if self.current_parameters is not None:\n            return self.current_parameters\n        return self.initial_parameters","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:57:35.744494Z","iopub.execute_input":"2025-04-07T00:57:35.744772Z","iopub.status.idle":"2025-04-07T00:57:35.765001Z","shell.execute_reply.started":"2025-04-07T00:57:35.744745Z","shell.execute_reply":"2025-04-07T00:57:35.764253Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualisations","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, roc_curve, auc\n\n# Add visualization functions before main()\ndef plot_training_metrics(history):\n    \"\"\"Plot training metrics from Flower history\"\"\"\n    if not history.metrics_distributed:\n        print(\"No metrics available in history\")\n        return\n    \n    plt.figure(figsize=(12, 5))\n    \n    # Training Loss\n    plt.subplot(1, 2, 1)\n    if \"train_loss\" in history.metrics_distributed:\n        losses = [metric[1] for metric in history.metrics_distributed[\"train_loss\"]]\n        plt.plot(losses, marker='o', label='Training Loss')\n    if \"test_loss\" in history.metrics_distributed:\n        losses = [metric[1] for metric in history.metrics_distributed[\"test_loss\"]]\n        plt.plot(losses, marker='o', label='Test Loss')\n    plt.title('Loss per Round')\n    plt.xlabel('Round')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # Accuracy\n    plt.subplot(1, 2, 2)\n    if \"train_accuracy\" in history.metrics_distributed:\n        accs = [metric[1] for metric in history.metrics_distributed[\"train_accuracy\"]]\n        plt.plot(accs, marker='o', label='Training Accuracy')\n    if \"test_accuracy\" in history.metrics_distributed:\n        accs = [metric[1] for metric in history.metrics_distributed[\"test_accuracy\"]]\n        plt.plot(accs, marker='o', label='Test Accuracy')\n    plt.title('Accuracy per Round')\n    plt.xlabel('Round')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:57:35.766757Z","iopub.execute_input":"2025-04-07T00:57:35.766957Z","iopub.status.idle":"2025-04-07T00:57:35.783325Z","shell.execute_reply.started":"2025-04-07T00:57:35.766940Z","shell.execute_reply":"2025-04-07T00:57:35.782578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def central_evaluation(model, test_loader, device):\n    \"\"\"Robust central evaluation with error handling\"\"\"\n    model.eval()\n    all_labels = []\n    all_preds = []\n    all_probs = []\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            try:\n                # Handle different batch formats\n                if isinstance(batch, (list, tuple)):\n                    data, targets = batch[0], batch[1]\n                else:  # Handle single-tensor batches\n                    data, targets = batch, None\n\n                if data is None:\n                    continue\n\n                # Move to device\n                inputs = data.to(device)\n                targets = targets.to(device) if targets is not None else None\n                \n                # Forward pass\n                outputs = model(inputs)\n                probs = torch.softmax(outputs, dim=1)\n                preds = torch.argmax(outputs, dim=1)\n                \n                # Only store if targets exist\n                if targets is not None:\n                    all_labels.extend(targets.cpu().numpy())\n                    all_preds.extend(preds.cpu().numpy())\n                    all_probs.extend(probs.cpu().numpy())\n                    \n            except Exception as e:\n                print(f\"Error processing batch: {str(e)}\")\n                continue\n    \n    return all_labels, all_preds, np.array(all_probs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:57:35.784541Z","iopub.execute_input":"2025-04-07T00:57:35.784727Z","iopub.status.idle":"2025-04-07T00:57:35.803940Z","shell.execute_reply.started":"2025-04-07T00:57:35.784711Z","shell.execute_reply":"2025-04-07T00:57:35.803163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_confusion_matrix(y_true, y_pred, class_names, normalize=False, figsize=(8, 6)):\n    \"\"\"\n    Plot a confusion matrix with enhanced visualization\n    \n    Parameters:\n    y_true (array): True labels\n    y_pred (array): Predicted labels\n    class_names (list): List of class names\n    normalize (bool): Whether to normalize the matrix\n    figsize (tuple): Figure size\n    \"\"\"\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Normalize if requested\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        fmt = '.2%'\n        title = 'Normalized Confusion Matrix'\n    else:\n        fmt = 'd'\n        title = 'Confusion Matrix'\n    \n    # Create figure\n    plt.figure(figsize=figsize)\n    \n    # Create heatmap\n    heatmap = sns.heatmap(\n        cm,\n        annot=True,\n        fmt=fmt,\n        cmap='Blues',\n        xticklabels=class_names,\n        yticklabels=class_names,\n        cbar=False,\n        linewidths=0.5,\n        annot_kws={'size': 12}\n    )\n    \n    # Add labels and title\n    plt.title(title, fontsize=14, pad=20)\n    plt.xlabel('Predicted Label', fontsize=12)\n    plt.ylabel('True Label', fontsize=12)\n    \n    # Adjust tick labels\n    plt.xticks(rotation=45, ha='right', fontsize=10)\n    plt.yticks(rotation=0, fontsize=10)\n    \n    # Add colorbar\n    plt.colorbar(heatmap.collections[0]).ax.set_ylabel('Counts' if not normalize else 'Percentage', \n                                                    rotation=270, labelpad=15)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return cm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:57:35.804736Z","iopub.execute_input":"2025-04-07T00:57:35.804966Z","iopub.status.idle":"2025-04-07T00:57:35.825274Z","shell.execute_reply.started":"2025-04-07T00:57:35.804937Z","shell.execute_reply":"2025-04-07T00:57:35.824693Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc, RocCurveDisplay\n\n\ndef plot_roc_curve(y_true, y_probs, class_names):\n    # Handle binary vs multi-class cases\n    n_classes = len(class_names)\n    \n    if n_classes == 2:\n        # Binary classification - use positive class probabilities\n        fpr, tpr, _ = roc_curve(y_true, y_probs[:, 1])  # Use second column\n        roc_auc = auc(fpr, tpr)\n        \n        plt.figure()\n        plt.plot(fpr, tpr, color='darkorange', lw=2,\n                 label=f'ROC curve (AUC = {roc_auc:.2f})')\n    else:\n        # Multi-class: One-vs-Rest\n        y_true_bin = label_binarize(y_true, classes=np.arange(n_classes))\n        \n        plt.figure()\n        for i in range(n_classes):\n            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_probs[:, i])\n            roc_auc = auc(fpr, tpr)\n            plt.plot(fpr, tpr, lw=2,\n                     label=f'{class_names[i]} (AUC = {roc_auc:.2f})')\n\n    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend(loc=\"lower right\")\n    plt.title('ROC Curve' + (' (Binary)' if n_classes == 2 else ' (OvR)'))\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:57:35.826144Z","iopub.execute_input":"2025-04-07T00:57:35.826451Z","iopub.status.idle":"2025-04-07T00:57:35.845586Z","shell.execute_reply.started":"2025-04-07T00:57:35.826403Z","shell.execute_reply":"2025-04-07T00:57:35.844992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import matplotlib.pyplot as plt\n# import torchaudio\n\n# def plot_comparison(raw, aug, sample_rate=16000):\n#     plt.figure(figsize=(18, 12))\n    \n#     # Time-align waveforms\n#     min_length = min(len(raw), len(aug))\n#     raw = raw[:min_length]\n#     aug = aug[:min_length]\n    \n#     # Create window function\n#     window = torch.hann_window(1024)\n    \n#     # Waveform plots with difference overlay\n#     plt.subplot(3, 1, 1)\n#     plt.plot(raw.numpy().squeeze(), 'b', alpha=0.6, label='Original')\n#     plt.plot(aug.numpy().squeeze(), 'r', alpha=0.4, label='Augmented')\n#     plt.plot(np.abs(raw.numpy().squeeze() - aug.numpy().squeeze()),'k', label='Difference')\n#     plt.xlabel(\"Time (s) (Samples)\")\n#     plt.ylabel(\"Amplitude\")\n#     plt.title(\"Waveform Comparison\")\n#     plt.legend()\n    \n#     # Spectral difference using correct parameters\n#     def create_spectrogram(waveform):\n#         return torchaudio.functional.spectrogram(\n#             waveform=waveform.unsqueeze(0),\n#             pad=0,\n#             window=window,\n#             n_fft=1024,\n#             hop_length=256,\n#             win_length=1024,\n#             power=2.0,\n#             normalized=False\n#         ).squeeze().log2().numpy()\n    \n#     S_raw = create_spectrogram(raw)\n#     S_aug = create_spectrogram(aug)\n    \n#     plt.subplot(3, 2, 3)\n#     plt.imshow(S_raw, aspect='auto', cmap='viridis', origin='lower')\n#     plt.xlabel(\"Time (s)\")\n#     plt.ylabel(\"Frequency (Hz)\")\n#     plt.title(\"Original Spectrogram\")\n#     plt.colorbar()\n    \n#     plt.subplot(3, 2, 4)\n#     plt.imshow(S_aug, aspect='auto', cmap='viridis', origin='lower')\n#     plt.xlabel(\"Time (s)\")\n#     plt.ylabel(\"Frequency (Hz)\")\n#     plt.title(\"Augmented Spectrogram\")\n#     plt.colorbar()\n    \n#     plt.subplot(3, 2, 5)\n#     plt.imshow(S_aug - S_raw, aspect='auto', cmap='coolwarm', origin='lower', vmin=-1, vmax=1)\n#     plt.xlabel(\"Time (s)\")\n#     plt.ylabel(\"Frequency (Hz)\")\n#     plt.title(\"Spectral Difference\")\n#     plt.colorbar()\n    \n#     plt.subplot(3, 2, 6)\n#     plt.specgram(raw.numpy().squeeze(), Fs=sample_rate, cmap='plasma', NFFT=1024, noverlap=512)\n#     plt.xlabel(\"Time (s)\")\n#     plt.ylabel(\"Frequency (Hz)\")\n#     plt.title(\"Original Spectrogram\")\n    \n#     plt.tight_layout()\n#     plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:57:35.846270Z","iopub.execute_input":"2025-04-07T00:57:35.846499Z","iopub.status.idle":"2025-04-07T00:57:35.865561Z","shell.execute_reply.started":"2025-04-07T00:57:35.846480Z","shell.execute_reply":"2025-04-07T00:57:35.864958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def get_sample_waveform(dataset, index=0):\n#     \"\"\"Helper to get raw and augmented versions of same sample\"\"\"\n#     # Create non-augmented version\n#     raw_dataset = AudioDataset(\n#         root_dir=dataset.config.dirs[\"train\"],\n#         config=dataset.config,\n#         augment=False\n#     )\n    \n#     # Get raw waveform\n#     raw_waveform, label = raw_dataset[index]\n    \n#     # Get augmented waveform (create new dataset with augment=True)\n#     aug_dataset = AudioDataset(\n#         root_dir=dataset.config.dirs[\"train\"],\n#         config=dataset.config,\n#         augment=True\n#     )\n#     aug_waveform, _ = aug_dataset[index]\n    \n#     return raw_waveform, aug_waveform\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:57:35.866205Z","iopub.execute_input":"2025-04-07T00:57:35.866395Z","iopub.status.idle":"2025-04-07T00:57:35.885172Z","shell.execute_reply.started":"2025-04-07T00:57:35.866378Z","shell.execute_reply":"2025-04-07T00:57:35.884475Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from IPython.display import Audio, display\n\n# def play_comparison(raw, aug, sr=16000):\n#     print(\"Original:\")\n#     display(Audio(raw.numpy().squeeze(), rate=sr))\n#     print(\"Augmented:\")\n#     display(Audio(aug.numpy().squeeze(), rate=sr))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:57:35.887448Z","iopub.execute_input":"2025-04-07T00:57:35.887638Z","iopub.status.idle":"2025-04-07T00:57:35.903904Z","shell.execute_reply.started":"2025-04-07T00:57:35.887622Z","shell.execute_reply":"2025-04-07T00:57:35.903232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ray.shutdown()\nray.init()\nos.environ[\"RAY_memory_monitor_refresh_ms\"]=\"0\"\n# os.environ[\"RAY_memory_usage_threshold\"] = \"0.98\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:57:35.905021Z","iopub.execute_input":"2025-04-07T00:57:35.905339Z","iopub.status.idle":"2025-04-07T00:57:40.063851Z","shell.execute_reply.started":"2025-04-07T00:57:35.905315Z","shell.execute_reply":"2025-04-07T00:57:40.062690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def weighted_avg(metrics, metric_name):\n    \"\"\"Helper function for metric aggregation\"\"\"\n    values = []\n    weights = []\n    for num_examples, m in metrics:\n        if metric_name in m:\n            values.append(m[metric_name])\n            weights.append(num_examples)\n    return sum(v * w for v, w in zip(values, weights)) / sum(weights) if weights else 0.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:57:40.064963Z","iopub.execute_input":"2025-04-07T00:57:40.065336Z","iopub.status.idle":"2025-04-07T00:57:40.071671Z","shell.execute_reply.started":"2025-04-07T00:57:40.065294Z","shell.execute_reply":"2025-04-07T00:57:40.070847Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Main function","metadata":{}},{"cell_type":"code","source":"from flwr.server.client_manager import SimpleClientManager  # Import missing class\nfrom torch.utils.data import random_split\n\ndef main():\n    try:\n        print(\"Starting federated learning setup\")\n        \n        # Load datasets\n        train_dataset = AudioDataset(config.dirs[\"train\"], config, augment = True)\n        train_size = int(1* len(train_dataset))\n        train_subset, _ = random_split(\n            train_dataset,\n            [train_size, len(train_dataset) - train_size],\n            generator=torch.Generator().manual_seed(42)\n        )\n\n        # if len(train_dataset) > 0:\n        #     # Get first sample's raw and augmented versions\n        #     raw_wave, aug_wave = get_sample_waveform(train_dataset, 2004)\n        #     # Plot comparison\n        #     plot_comparison(raw_wave, aug_wave)\n        #     play_comparison(raw_wave, aug_wave, config.sample_rate)\n        # else:\n        #     print(\"Dataset is empty - check your data paths!\")\n            \n        test_dataset = AudioDataset(config.dirs[\"test\"], config)\n        test_loader = DataLoader(test_dataset,\n                                 batch_size=config.batch_size,\n                                 collate_fn=collate_fn,\n                                 shuffle=False)\n\n        # Create metrics collector\n        metrics_collector = DiskMetricsCollector()\n        \n        # Split into client partitions\n        indices = list(range(len(train_subset)))\n        chunk_size = len(indices) // config.num_clients\n        \n        # Create client datasets\n        client_datasets = [\n            Subset(train_subset, indices[i*chunk_size:(i+1)*chunk_size])\n            for i in range(config.num_clients)\n        ]\n\n        print(f\"Using {len(train_subset)}/{len(train_dataset)} training samples\")\n        print(f\"Test samples: {len(test_dataset)}\")\n\n        # Define client creation function\n        def client_fn(cid: str) -> FlowerClient:\n            \"\"\"Create client with fresh model instance\"\"\"\n            client_id = int(cid)\n            torch.cuda.empty_cache()\n            model = HuBERTClassifier(config).to(config.device)  # Ensure model is on correct device\n            numpy_client = FlowerClient(\n                model=model,\n                train_data=client_datasets[client_id],\n                test_data=test_dataset,\n                config=config,\n                client_id=client_id,\n                metrics_collector=metrics_collector\n            )\n            return numpy_client.to_client()\n\n        # Initialize global model\n        initial_model = HuBERTClassifier(config).to(config.device)\n        initial_params = fl.common.ndarrays_to_parameters([\n            val.cpu().numpy() for _, val in initial_model.state_dict().items()\n        ])\n\n        # Configure strategy with proper aggregation\n        strategy = CustomFedAvg(\n            metrics_collector=metrics_collector,\n            min_fit_clients=config.num_clients,\n            min_available_clients=config.num_clients,\n            initial_parameters=initial_params,\n            fit_metrics_aggregation_fn=lambda metrics: {\n                \"train_loss\": weighted_avg(metrics, \"loss\"),\n                \"train_accuracy\": weighted_avg(metrics, \"accuracy\")\n            },\n            evaluate_metrics_aggregation_fn=lambda metrics: {\n                \"test_loss\": weighted_avg(metrics, \"loss\"),\n                \"test_accuracy\": weighted_avg(metrics, \"accuracy\")\n            },\n\n        )\n\n        print(\"Starting federated training\")\n        history = fl.simulation.start_simulation(\n            client_fn=client_fn,\n            num_clients=config.num_clients,\n            config=fl.server.ServerConfig(num_rounds=config.num_rounds),\n            strategy=strategy,\n            client_resources={\n                \"num_cpus\": 0.8,\n                \"num_gpus\": 0.25 if torch.cuda.is_available() else 0\n            },\n        )\n\n        print(\"\\nGenerating visualizations...\")\n        for round_num in range(1, config.num_rounds+1):\n            metrics_collector.plot_round(round_num)\n            print(\"\\nPlotting training metrics...\")\n            plot_training_metrics(history)\n\n        print(\"\\nPerforming central evaluation...\")\n        final_model = HuBERTClassifier(config).to(config.device)\n        params_obj = strategy.get_parameters(None)\n        \n        if params_obj is None:\n            print(\"Using initial parameters for evaluation\")\n            params_obj = initial_params\n            raise ValueError(\"No parameters found in strategy\")\n        final_params = fl.common.parameters_to_ndarrays(params_obj)\n\n        # Load parameters into the model\n        sd = final_model.state_dict()\n        param_names = [name for name, _ in final_model.named_parameters()]\n        assert len(final_params) == len(param_names), \\\n            f\"Parameter count mismatch: Model has {len(param_names)}, strategy supplied {len(final_params)}\"\n        \n        for (name, _), array in zip(final_model.named_parameters(), final_params):\n            sd[name] = torch.from_numpy(array).to(config.device)\n\n        final_model.load_state_dict(sd, strict=True)\n        final_model = final_model.to(config.device)\n\n        # Device verification\n        print(\"\\n=== Device Verification ===\")\n        print(f\"Model device: {next(final_model.parameters()).device}\")\n        sample_batch = next(iter(test_loader))[0]\n        print(f\"Data device: {sample_batch.device}\")\n\n        try:\n            y_true, y_pred, y_probs = central_evaluation(final_model, test_loader, config.device)\n        except Exception as e:\n            print(f\"Evaluation failed: {str(e)}\")\n            print(\"Model architecture:\", final_model)\n            print(\"Sample input shape:\", sample_batch.shape)\n            raise\n        \n        # Move test data to correct device\n        for batch in test_loader:\n            inputs, labels = batch\n            inputs = inputs.to(config.device)\n            labels = labels.to(config.device)\n\n         # Handle binary probabilities\n        print(f\"\\nProbability matrix shape: {y_probs.shape}\")\n        if y_probs.shape[1] == 1:\n            y_probs = np.hstack([1 - y_probs, y_probs])\n\n        class_names = [\"Real\", \"Fake\"]  # Define your classes\n        plot_confusion_matrix(y_true, y_pred, class_names)\n        plot_roc_curve(y_true, y_probs, class_names)\n\n        \n        from sklearn.metrics import classification_report\n        import pandas as pd\n        \n        print(\"\\n=== Class-wise Performance Metrics ===\")\n        report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n        report_df = pd.DataFrame(report).transpose()\n        print(\"===full report===\\n\")\n        print(report_df)\n        print(\"\\n===selected report===\\n\")\n        print(report_df[[\"precision\", \"recall\", \"f1-score\"]].round(2))\n\n        return history\n\n    except Exception as e:\n        print(f\"Critical failure: {str(e)}\")\n        raise\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:57:40.072347Z","iopub.execute_input":"2025-04-07T00:57:40.072639Z","iopub.status.idle":"2025-04-07T00:57:40.096168Z","shell.execute_reply.started":"2025-04-07T00:57:40.072611Z","shell.execute_reply":"2025-04-07T00:57:40.095289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    history = main()  # Return history from main()\n    # Additional analysis using history can happen here","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T00:57:40.097072Z","iopub.execute_input":"2025-04-07T00:57:40.097368Z","execution_failed":"2025-04-07T01:24:01.958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Access metrics directly\n# all_metrics = ray.get(metrics_collector.metrics.remote())\n\n# # Plot specific round\n# ray.get(metrics_collector.plot_round_metrics.remote(3))  # Plot round 3\n\n# # Plot client 0's progress\n# ray.get(metrics_collector.plot_client_progress.remote(0))\n# # Load parameters from round 5\n\n# params = torch.load(\"round_5_params.pth\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-07T01:24:01.958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ray.shutdown()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-07T01:24:01.958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# m=DiskMetricsCollector()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-07T01:24:01.958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}